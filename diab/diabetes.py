# -*- coding: utf-8 -*-
"""diabetes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19QoZi6yTl7so7u0Q1dBfjTOPUhZsxXu-
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
#import matplotlib.pyplot as plt


# %matplotlib inline
path = r"â€ªD://Project//Minor_Project//model//diab//diabetes.csv"
data = pd.read_csv(path)
type(data)

feature_columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']
predicted_class = ['Outcome']

data.shape

data.head(5)

data.isnull().values.any()

covmatrix = data.corr()
corr_features = covmatrix.index
import seaborn as sns
#import matplotlib.pyplot as plt
plt.figure(figsize=(20,20))
g=sns.heatmap(data[corr_features].corr(),annot=True,cmap="RdYlGn")

covmatrix

diabetes_true_count = len(data.loc[data['Outcome'] == True])
diabetes_false_count = len(data.loc[data['Outcome'] == False])

(diabetes_true_count,diabetes_false_count)

feature_columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']
predicted_class = ['Outcome']

"""# Handling Missing Values"""

print("total number of rows : {0}".format(len(data)))
def chck_miss(data,feature_columns):
  print("Pregnancies can be Zero")
  for i in feature_columns:
    print("Number of rows missing "+str(i)+": {0}".format(len(data.loc[data[i] == 0])))
chck_miss(data,feature_columns)

#print(type(data))
from sklearn.impute import SimpleImputer

fill_values = SimpleImputer(missing_values=0, strategy="mean")

cols_backup = feature_columns+predicted_class
temp_preg = data[["Pregnancies","Outcome"]].copy()
#print("Datatype:",type(data))
data = fill_values.fit_transform(data)
data = pd.DataFrame(data=data, columns=cols_backup)
data[['Pregnancies','Outcome']] = temp_preg[['Pregnancies','Outcome']]

chck_miss(data,feature_columns)

data.head()

"""# Training and Testing"""

from sklearn.model_selection import train_test_split

X = data[feature_columns]
y = data[predicted_class]

#X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, random_state=1)

from sklearn.preprocessing import StandardScaler
def preprocess_inputs(df):
  df = df.copy()
  scaler = StandardScaler()
  scaler.fit(df)
  df = pd.DataFrame(scaler.transform(df), index = df.index, columns = df.columns)
  return df,scaler

X,scaler = preprocess_inputs(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, random_state=1)

X_train

X_test

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import LinearSVC, SVC
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
models = {
    "Logistic Regression"                   : LogisticRegression(),
    "K-Nearest Neighbors"                   : KNeighborsClassifier(),
    "Decision Tree"                         : DecisionTreeClassifier(),
    "Support Vector Machine (Linear Kernel)": LinearSVC(max_iter=1500),
    "Support Vector Machine (RBF Kernel)"   : SVC(),
    "Neural Network"                        : MLPClassifier(max_iter=1500),
    "Random Forest"                         : RandomForestClassifier(),
    "Gradient Boosting"                     : GradientBoostingClassifier()
}

for name, model in models.items():
    model.fit(X_train, y_train)
    print(name + " trained.")

for name, model in models.items():
  print(name + ": {:.2f}%".format(model.score(X_test, y_test) * 100))

rf = models.get("Random Forest")

from sklearn.metrics import confusion_matrix,accuracy_score,classification_report
pred = rf.predict(X_test)

cm = confusion_matrix(y_test,pred)
print(cm)

print("Accuracy:", accuracy_score(y_test,pred))
print(classification_report(y_test,pred))

'''
def op_pred(filename,rf):
  df = pd.read_csv(filename)
  df = preprocess_inputs(df)
  pred = rf.predict(df)
  #pred
  result = []
  for row in range(df.shape[0]):
    for (intercept, coef) in zip(rf.intercept_, rf.coef_):
        s = "y = {0:.3f}".format(intercept)
        h = intercept
        mx = -1
        for (i, c) in enumerate(coef):
            s += " + {0:.3f} * x{1}".format(c, i)
            k = max(h,c*df.iloc[row][i])
            if(k>h):
              h = k
              mx = i
        #print(s)
        #print(h,mx)
        #print(pred[row],df.columns[mx])
        if(pred[row]==0):
          result.append("No Trace of Diabeties Detected")
        else:
          result.append("Diabeties Detected "+str(df.columns[mx])+"'")

  return result'''

def preprocess_outputs(df,scaler):
  df = df.copy()
  df = pd.DataFrame(scaler.transform(df), index = df.index, columns = df.columns)
  return df

def op_pred(filename,rf,scaler):
  df = pd.read_csv(filename)
  df = preprocess_outputs(df,scaler)
  pred = rf.predict(df)
  res = []
  for row in range(df.shape[0]):
    if(pred[row]==0):
      res.append("No Trace of Diabetes Detected")
    else:
      res.append("Please consult a doctor for further process!")

import pickle
# open a file, where you ant to store the data
file = open('diab_rf.pkl', 'wb')

# dump information to that file
pickle.dump(rf, file)

# open a file, where you ant to store the data
file = open('scaler.pkl', 'wb')

# dump information to that file
pickle.dump(scaler, file)
